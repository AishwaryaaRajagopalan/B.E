from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import plot_tree

#Load the wine dataset
data = datasets.load_wine(as_frame = True)
X = data.data
y = data.target

#Split the dataset into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 22)

#Train a Decision Tree Classifier
dtree = DecisionTreeClassifier(random_state = 22)
dtree.fit(X_train,y_train)

#Predict on the test set
y_pred = dtree.predict(X_test)

#Print accuracy scores
print ("Train data accuracy:",accuracy_score(y_true = y_train, y_pred = dtree.predict(X_train)))
print ("Test data accuracy:",accuracy_score(y_true = y_test, y_pred = y_pred))

#Performance evaluation of bagging
estimator_range = [2,4,6,8,10,12,14,16]
models = []
scores = []

for n_estimators in estimator_range:
 # Create bagging classifier
    clf = BaggingClassifier(n_estimators = n_estimators, random_state = 22)
 # Fit the model
    clf.fit(X_train, y_train)
 # Append the model and score to their respective list
    scores.append(accuracy_score(y_true = y_test, y_pred = clf.predict(X_test)))

#Generate the plot of scores against number of estimators
plt.figure(figsize=(9,6))
plt.plot(estimator_range, scores)

# Adjust labels and font (to make vispble)
plt.xlabel("n_estimators", fontsize = 18)
plt.ylabel("score", fontsize = 18)
plt.tick_params(labelsize = 16)

# Visualize plot
plt.show()
# Generate Decision Trees from Bagging Classifier
clf = BaggingClassifier(n_estimators=12, oob_score=True, random_state=22)
clf.fit(X_train, y_train)

plt.figure(figsize=(30, 20))
plot_tree(clf.estimators_[0], feature_names=data.feature_names)
plt.show()
